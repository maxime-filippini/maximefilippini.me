<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover"><link rel="stylesheet" href="/stylesheets/hljs.css"><link rel="stylesheet" href="/stylesheets/styles.css"><title>Some tips when working with pandas</title><link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;ðŸ–¥&lt;/text&gt;&lt;/svg&gt;"><script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.10.0/languages/python.min.js"></script><script>hljs.highlightAll();</script><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&amp;display=swap"></head><body class="bg-bg text-white font-roboto"><div class="container max-w-3xl mx-auto flex flex-col"><nav class="w-full py-4 sticky top-0 bg-bg border-b-2 border-surface-0"><ul class="flex gap-8 justify-center"><li class="text-2xl text-[#a6d189]"><a href="/">Maxime Filippini</a></li><div class="w-4 border-r-2 mr-4"></div><li class="text-lg"><a href="/blog/index.html">Blog</a></li><li class="text-lg"><a href="/about.html">About</a></li></ul></nav><div class="p-4"><h1 class="text-5xl mb-6 mt-6 font-bold">Some tips when working with pandas</h1><div class="text-lg"><p class="mb-3">Most of us working in Python do so because of the powerful set of libraries that have been built for it. One such library I would like to discuss today is the ubiquitous <code>pandas</code>.</p><p class="mb-3">For the two people who have not heard of <code>pandas</code>, it allows you to:</p><ul class="list-disc list-outside ml-4"><li><p class="mb-3">Read tabular data from a multitude of sources (e.g. csv, Excel, databases);</p></li><li><p class="mb-3">Clean up that data;</p></li><li><p class="mb-3">Manipulate it to fit your need (e.g. adding columns defined by formulas, grouping and aggregations);</p></li><li><p class="mb-3">Display your data as plots; and</p></li><li><p class="mb-3">Export your data to any format you would like.</p></li></ul><p class="mb-3">In this post, we are going to discuss some dos and don&#39;ts when working with <code>pandas</code>. If any reader wants to dig deeper into these best practices, I recommend the excellent book <a href="https://store.metasnake.com/effective-pandas-book">Effective Pandas - Patterns for Data Manipulation</a> by Matt Harrison, which was the inspiration for this post.</p><h2 class="text-3xl font-bold my-6">The concept of &quot;tidy data&quot;</h2><p class="mb-3">If you have worked with the <a href="https://www.r-project.org/">R language</a>, chance is you are familiar with the concept of tidy data, since it is the back-bone of the very popular collection of packages: the <a href="https://www.tidyverse.org/">tidyverse</a>.</p><p class="mb-3">In a <a href="https://vita.had.co.nz/papers/tidy-data.pdf">paper</a> published in the Journal of Statistical Software in 2014, Hadley Wickham (creator of the tidyverse) details the main tenets of so-called &quot;tidy data&quot;, and the benefits they bring to our data analysis work. The concept of tidy data is language-agnostic, so we can apply it in our work with the <code>pandas</code> library.</p><p class="mb-3">A table holds <strong>values</strong>, each of which belongs to an <strong>observation</strong> as well as a <strong>variable</strong>. In a given dataset, an observation contains all of the measurements for a given unit across attributes (e.g. a financial instrument), while a variable contains all measurements for the same underlying attribute (e.g. price).</p><p class="mb-3">We would consider a dataset &quot;tidy&quot; if the following is true:</p><ul class="list-disc list-outside ml-4"><li><p class="mb-3">Each <strong>variable</strong> forms a column;</p></li><li><p class="mb-3">Each <strong>observation</strong> forms a row; and</p></li><li><p class="mb-3">Each type of observational unit forms a table.</p></li></ul><p class="mb-3">But why should we bother having tidy data? Let us consider the example of the data retrieved from the Yahoo Finance API using the following code:</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">import yfinance as yf

df_prices = (
    yf
    .Tickers(&quot;MSFT AAPL TSLA&quot;)
    .download(period=&quot;5d&quot;)
    .loc[:, &quot;Close&quot;]
)

print(df_prices)</code></pre><p class="mb-3">Now what is wrong with that? Well first of all, the data is poorly labeled, as it is not clear at first glance what type of values are included in this table. To illustrate the second reason as to why there may be issues with this format, let us consider the following exercise:</p><blockquote><p class="mb-3">We want to compute the price of a portfolio made up of these three stocks, with quantities provided in a separate dataframe <code>df_quantities</code>, as shown below.</p></blockquote><p class="mb-3">One way to do so would be to merge the two dataframes using suffixes...</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_mrg = df_prices.merge(
    df_quantities,
    left_index=True,
    right_index=True,
    suffixes=(&quot;_PX&quot;, &quot;_QT&quot;)
)

print(df_mrg)</code></pre><p class="mb-3">... and combine the columns appropriately</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_ptf = pd.DataFrame(
  index=df_mrg.index,
    data={
        &quot;value&quot;: (
            df_mrg[&quot;AAPL_PX&quot;] * df_mrg[&quot;AAPL_QT&quot;]
            + df_mrg[&quot;MSFT_PX&quot;] * df_mrg[&quot;MSFT_QT&quot;]
            + df_mrg[&quot;TSLA_PX&quot;] * df_mrg[&quot;TSLA_QT&quot;]
        )
    }
)

print(df_ptf)</code></pre><p class="mb-3">But this wouldn&#39;t work very well for portfolios made up of other stocks, or that include more than 3 positions. To generalize it, one must think of the process used to build this new value column:</p><blockquote><p class="mb-3">For each <strong>date/stock</strong> pair, multiply the <strong>quantity</strong> and the **price **to obtain the position value. Then, for each <strong>date</strong>, sum these values to obtain the portfolio value.</p></blockquote><p class="mb-3">Clearly, in this context, the <strong>variables</strong> of our input table are <em>quantity</em> and *price, *while each date/stock pair forms an <strong>observation</strong>. To make the original dataset tidy, we use the <code>.melt</code> method:</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_prices_tidy = (
    pd.melt(
        df_prices.reset_index(),
        id_vars=[&quot;Date&quot;],
        var_name=&quot;stock&quot;,
        value_name=&quot;price&quot;,
    )
    .set_index([&quot;Date&quot;, &quot;stock&quot;])
)

print(df_prices_tidy)</code></pre><p class="mb-3">The same can be done with the table of quantities, and the merge operation now yields:</p><p class="mb-3">Then, the value can be calculated for each observation, and an aggregation per date can be performed via the <code>groupby</code> and <code>agg</code> methods.</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_values = (
    df_mrg_tidy

    # We create a new column to store the position values
    .assign(value=lambda df: df[&quot;price&quot;] * df[&quot;quantity&quot;])

  # For each date, sum the &quot;value&quot; column and store into &quot;ptf_value&quot;
    .groupby(&quot;Date&quot;)
    .agg(ptf_value=(&quot;value&quot;, &quot;sum&quot;))
)</code></pre><p class="mb-3">With tidy data, the data manipulation steps make sense from a semantic point of view. To obtain the portfolio value for each date, we first compute the value of each position at each date, using prices and quantities, and then we aggregate these values via a sum for each date in our table.</p><p class="mb-3">But how do we transform data to become tidy? The <a href="https://vita.had.co.nz/papers/tidy-data.pdf">paper</a> shows the techniques in detail, but mostly, it is about &quot;pivoting&quot; (i.e. moving from a &quot;long&quot; table to a &quot;wide&quot; table) and &quot;melting&quot; (the opposite operation, which we used earlier). In pandas, the methods which can be used are <code>.pivot</code>, <code>.pivot_table</code>, and <code>.melt</code> (which we used above).</p><h2 class="text-3xl font-bold my-6">Chaining</h2><p class="mb-3">If you have worked with <code>pandas</code> code before, you may have seen data manipulation procedures that are a tad difficult to read because they are written in a procedural style, assigning values to temporary variables along the way.</p><p class="mb-3">Consider our example above. In most code, you may see that procedure written like below:</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_mrg_tidy[&quot;value&quot;] = df_mrg_tidy[&quot;price&quot;] * df_mrg_tidy[&quot;quantity&quot;]
df_grp = df_mrg_tidy.groupby(&quot;Date&quot;)
df_values = df_grp.agg(ptf_value=(&quot;value&quot;, &quot;sum&quot;))</code></pre><p class="mb-3">Here, we are messing with <code>df_mrg_tidy</code> by adding a column that only has value after aggregation as part of <code>df_values</code>, which is not something we want.</p><p class="mb-3">A better approach is to perform our data manipulation in a single step, starting from a starting dataset and producing the result we need. This style is close to the main tenet of <a href="https://en.wikipedia.org/wiki/Functional_programming">functional programming</a>, which is the lack of <a href="https://en.wikipedia.org/wiki/Side_effect_(computer_science)">side effects</a>.</p><p class="mb-3">To do so, we use the <strong>chaining</strong> method, which takes advantage of the fact that pandas methods usually return the modified dataframe, which means that further methods can then be called on it. Recall our previous example, which uses the <code>assign</code> method to create a column, rather than the <code>df[col] = ...</code> pattern used above.</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_values = (
    df_mrg_tidy

    # We create a new column to store the position values
    .assign(value=lambda df: df[&quot;price&quot;] * df[&quot;quantity&quot;])

  # For each date, sum the &quot;value&quot; column and store into &quot;ptf_value&quot;
    .groupby(&quot;Date&quot;)
    .agg(ptf_value=(&quot;value&quot;, &quot;sum&quot;))
)</code></pre><p class="mb-3">The flow is now pretty clear: we start from <code>df_mrg_tidy</code> to produce <code>df_values</code>. The steps to get there are the creation of a new column <code>value</code>, and an aggregation of that column via a sum, for each unique value in the <code>Date</code> column.</p><h4>Ceci n&#39;est pas une </h4><p class="mb-3">Consider we have access to the following function that parses the date columns of a dataframe for a given format (I have deliberately not written it as a chain so as to avoid confusion).</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">def parse_date_columns(
  df: pd.DataFrame,
    columns: list[str],
    format: str
) -&gt; pd.DataFrame:
  df_out = df.copy()

    for col in columns:
      df_out.loc[:, col] = pd.to_datetime(df_out[col], format=format)

    return df_out</code></pre><p class="mb-3">Applying a function such as this one cannot be done directly when chaining, as... well the following does not quite look like a chain.</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_clean = (
  parse_date_columns(
      (
            df_start
            .rename(columns={&quot;Date&quot;: &quot;date&quot;, &quot;Values&quot;: &quot;value&quot;})
            .dropna()
            .assign(squares=lambda df: df[&quot;value&quot;] ** 2)
        ),
        columns=[&quot;date&quot;],
        format=&quot;%Y-%m-%d&quot;,
    )
)</code></pre><p class="mb-3">It naturally becomes more convoluted when multiple functions like this one are involved in our data manipulation process.</p><p class="mb-3">To integrate these functions into our data processing chains, we pass them as arguments to the ever-so-handy <code>.pipe</code> method. The above code is therefore equivalent to:</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_clean = (
  df_start
    .rename(columns={&quot;Date&quot;: &quot;date&quot;, &quot;Values&quot;: &quot;value&quot;})
    .dropna()
    .assign(squares=lambda df: df[&quot;value&quot;] ** 2)
    .pipe(parse_date_columns, columns=[&quot;date&quot;], format=&quot;%Y-%m-%d&quot;)
)</code></pre><p class="mb-3">Note that, in some situations, we may want to use <code>.pipe</code> even when pandas methods are available to us. Consider the simple example of a simple filtering on rows:</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_filtered = (
  df_start
    .loc[df_start[&quot;value&quot;] &gt; 3]
    .dropna()
)</code></pre><p class="mb-3">Since we refer to <code>df_start</code> in line 3, we cannot easily switch around the order of operations. Indeed, swapping line 3 and 4 will not give the same result if <code>.dropna()</code> ends up removing rows, since <code>df_start</code> refers to the starting state of our dataframe. I usually like to avoid this situation, to ensure that my pipelines are actually doing what I expect them to do, but also to make them somewhat reusable.</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_filtered = (
  df_start
    .pipe(lambda df: df.loc[df[&quot;value&quot;] &gt; 3])
    .dropna()
)</code></pre><p class="mb-3">Now, we can flip the operations over and get the same results, and we could hypothetically copy that line 3 and paste it in another pipeline and it would work all the same, as long as a <code>value</code> column is present in that new dataframe. An alternative to this would have been the <code>.query</code> method, as shown below.</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_filtered = (
  df_start
    .query(&quot;Value &gt; 3&quot;)
    .dropna()
)</code></pre><p class="mb-3">Chaining, overall, allows for cleaner data operations that <strong>read like a recipe</strong>. Indeed, the sequential order makes it much more readable, and ultimately reusable. As shown in one of our previous examples, you can also add comments inside your chain to improve the readability even more.</p><p class="mb-3">One last advantage of chaining, is that it makes the dreaded <code>SettingWithCopyWarning</code> basically disappear, since we are not modifying existing dataframes on every line of code.</p><h2 class="text-3xl font-bold my-6">Take advantage of vectorization</h2><p class="mb-3">One of the main reasons for the popularity of libraries like <code>pandas</code> and <code>numpy</code> is the combination of a convenient python API and a powerful calculation engine. However, there are <a href="https://en.wikipedia.org/wiki/Anti-pattern">anti-patterns</a> when working with <code>pandas</code>, some that I have seen time and time over.</p><p class="mb-3">First of all, while you can technically loop on <code>pandas</code> dataframes via methods such as <code>.iterrows</code> and <code>.itertuples</code>, it is incompatible with chaining and is overly slow and verbose on top of it. I have yet to find a situation where I would have to resort to using <code>for</code> loops on dataframes using these methods, especially given the existence of the <code>.apply</code> method.</p><p class="mb-3"><code>.apply</code> is convenient because it lets you apply python functions to elements in your dataframe, be it rows or individual values, while retaining the ability to use chaining. However, it should be a last resort and specific methods should be preferred, when those exist.</p><p class="mb-3">Let us see a simple example, where we try to determine the even numbers from random integers stored in a dataframe.</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">import timeit

setup_str = &quot;&quot;&quot;
import pandas as pd
import numpy as np

df = pd.DataFrame(
    data={
        &quot;value&quot;: np.random.random_integers(
            low=0,
            high=5000,
            size=(100_000,)
        )
    }
)
&quot;&quot;&quot;

with_apply = &quot;df[&#39;value&#39;].apply(lambda x: x % 2 == 0)&quot;
with_method = &quot;df[&#39;value&#39;].mod(2) == 0&quot;

print(timeit.timeit(with_apply, setup=setup_str, number=100))
print(timeit.timeit(with_method, setup=setup_str, number=100))</code></pre><p class="mb-3">The simple reason for the significantly slower execution of the approach using <code>.apply</code> is that the function <strong>loops</strong> over each row of the dataframe. In <code>pandas</code>, looping should be a last resort, and it is generally not required.</p><p class="mb-3">Let us consider a slightly less contrived example, where we want to create a column based on some criteria. Here, our alternative is <code>np.select</code>, which takes a list of arrays of booleans (i.e. True/False) and a list of outputs, and recombines them into a single array.</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">setup_str = &quot;&quot;&quot;
import pandas as pd
import numpy as np

def greatest(a, b):
    if a &gt; b:
        return &quot;a&quot;
    elif b &gt; a:
        return &quot;b&quot;
    return None

df = pd.DataFrame(
    data={
        &quot;a&quot;: np.random.random_integers(
            low=0,
            high=5000,
            size=(100_000,)
        ),
        &quot;b&quot;: np.random.random_integers(
            low=0,
            high=5000,
            size=(100_000,)
        )
    }
)
&quot;&quot;&quot;

with_apply = &quot;&quot;&quot;df.assign(
  greatest=lambda df_: df_.apply(
      lambda row: greatest(**row), axis=1))
&quot;&quot;&quot;

with_method = &quot;&quot;&quot;
df.assign(
    odd_even=lambda df_: np.select(
        condlist=[
            df_[&#39;a&#39;] &gt; df_[&#39;b&#39;],
            df_[&#39;b&#39;] &gt; df_[&#39;a&#39;],
        ],
        choicelist=[
            &#39;a&#39;,
            &#39;b&#39;,
        ],
        default=None,
    )
)
&quot;&quot;&quot;

print(timeit.timeit(with_apply, setup=setup_str, number=100))
print(timeit.timeit(with_method, setup=setup_str, number=100))</code></pre><p class="mb-3">A good question to ask yourself when using <code>.apply</code>, is whether there is any other solution that is built around vectors/arrays rather than individual values, as those solutions would in general be more performant. These solutions can either be dataframe methods, or <code>numpy</code> functions, like <code>where</code> or <code>select</code>.</p><h2 class="text-3xl font-bold my-6">Some snippets</h2><p class="mb-3">Here is a couple of real-world snippets of processing pipelines below, which you may find useful.</p><h6>Concatenation of multiple tables stored in .csv files, with some light processing.</h6><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">from datetime import datetime

df_loaded = (
  pd.concat(
      [
          (
                pd.read_csv(
                  f&quot;./data_{date}.csv&quot;,
                    sep=&quot;;&quot;,
                    decimal=&quot;,&quot;
                )
                .dropna()
                .assign(file_date=datetime.strptime(date, &quot;%Y%m%d&quot;))

            )
            for date in [&quot;20220930&quot;, &quot;20220831&quot;, &quot;20220731&quot;]
        ],
        axis=0
    )

)</code></pre><p class="mb-3">ðŸ’¡</p><p class="mb-3">Loading multiple tables into a single dataframe is faster using <code>pd.concat</code> rather than appending.</p><h5>Assignment using a dictionary</h5><p class="mb-3">What if we want to replace <code>NaN</code> values in a certain set of columns while chaining?</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_processed = (
  df_start
    .pipe(lambda df: df.assign(
      **{
          col: df[col].fillna(value=0)
          for col in [&quot;price&quot;, &quot;quantity&quot;, &quot;dividend&quot;]
        }
    ))
)</code></pre><p class="mb-3">Here, we unpack a dictionary which holds the columns to be assigned as keys, and the processed pandas series as values. This is necessary because the columns are defined programmatically.</p><p class="mb-3">If you are not aware of what dictionary unpacking does, it transforms the dictionary into keyword arguments when the function is executed. For example, <code>f(**{&quot;a&quot;: 2, &quot;b&quot;: 3})</code> becomes <code>f(a=2, b=3)</code>.</p><h5>Apply functions on groups</h5><p class="mb-3">Previously, I have mentioned that the <code>.apply</code> method should be avoided. One exception is when applied right after a <code>.groupby</code> operation. Indeed, after grouping, using <code>.apply</code> allows you to apply a function that takes a dataframe as an input and returns a dataframe to each group, which is of great use for chaining.</p><p class="mb-3">See the example below, where we wish to transform the <code>value</code> column into a percentage for each given <code>date</code>.</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_processed = (
  df_start
    .groupby(&quot;date&quot;, group_keys=False)
    .apply(
      lambda df: df.assign(pct=df[&quot;value&quot;] / df[&quot;value&quot;].sum())
    )
)</code></pre><p class="mb-3">Note that this could have been solved differently using a <code>merge</code></p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">df_processed = (
  df_start
    .pipe(
        lambda df: df.merge(
            (
                df_start
                .groupby(&quot;date&quot;)
                .agg(total=(&quot;value&quot;, &quot;sum&quot;))
                .reset_index()
            ),
            on=&quot;date&quot;
        )
    )
    .assign(pct=lambda df: df[&quot;value&quot;] / df[&quot;total&quot;])
  .drop(columns=[&quot;total&quot;])
)</code></pre><h5>Debugging chains</h5><p class="mb-3">One thing you may wonder is how do we go about debugging chains. One simple way of doing so is to implement your own diagnostic function, to call with <code>.pipe</code> along your process.</p><pre class="my-6 bg-surface-0 p-4 rounded-md overflow-x-scroll"><code data-lang="python" class="not-prose language-python">def show(df):
  print(df.head())
    return df

df_debug = (
    df_mrg_tidy

    # We create a new column to store the position values
    .assign(value=lambda df: df[&quot;price&quot;] * df[&quot;quantity&quot;])

    # Useful for debug
    .show()

  # For each date, sum the &quot;value&quot; column and store into &quot;ptf_value&quot;
    .groupby(&quot;Date&quot;)
    .agg(ptf_value=(&quot;value&quot;, &quot;sum&quot;))
)</code></pre><h2 class="text-3xl font-bold my-6">Conclusion</h2><p class="mb-3">I hope you have found those tips useful, and that you will be able to implement them in your processing pipelines. This is of course only scratching the surface, and I would recommend you to read Matt Harrison&#39;s book.</p><p class="mb-3">As with everything, practice makes perfect, so I encourage you to build pipelines, and look at them critically in light of the elements mentioned.</p><p class="mb-3">Stay tuned for more!</p></div></div></div></body></html>